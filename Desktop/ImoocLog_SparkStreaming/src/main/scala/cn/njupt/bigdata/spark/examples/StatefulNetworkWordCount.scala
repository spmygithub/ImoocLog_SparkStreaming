package cn.njupt.bigdata.spark

import org.apache.spark.SparkConf
import org.apache.spark.streaming._

/**
  * 使用Spark Streaming完成有状态统计
  */
object StatefulNetworkWordCount {
  val sparkConf = new SparkConf().setAppName("StatefulNetworkWordCount")
  // Create the context with a 1 second batch size
  val ssc = new StreamingContext(sparkConf, Seconds(1))

  // 如果使用了stateful的算子，必须要设置checkpoint
  // 在生产环境中，建议大家把checkpoint设置到HDFS的某个文件夹中
  ssc.checkpoint("hdfs///opt/spark/data/checkpoint")

  // Initial state RDD for mapWithState operation
  val initialRDD = ssc.sparkContext.parallelize(List(("hello", 1), ("world", 1)))

  // Create a ReceiverInputDStream on target ip:port and count the
  // words in input stream of \n delimited test (eg. generated by 'nc')
  val lines = ssc.socketTextStream("localhost",6789)
  val words = lines.flatMap(_.split(" "))
  val wordDstream = words.map(x => (x, 1))

  // Update the cumulative count using mapWithState
  // This will give a DStream made of state (which is the cumulative count of the words)
  val mappingFunc = (word: String, one: Option[Int], state: State[Int]) => {
    val sum = one.getOrElse(0) + state.getOption.getOrElse(0)
    val output = (word, sum)
    state.update(sum)
    output
  }

  val stateDstream = wordDstream.mapWithState(
    StateSpec.function(mappingFunc).initialState(initialRDD))
  stateDstream.print()
  ssc.start()
  ssc.awaitTermination()
}
